# -*- coding: utf-8 -*-
"""setup_server.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p4woQF1h9WXJQ1DrT3kccjr_-MO5jUFZ
"""

!pip install dotenv
!pip install textstat

# ========== Standard Library ==========
import os
import sys
import json
import random
import subprocess
import warnings
from argparse import ArgumentParser
from pathlib import Path
from itertools import product

# ========== Environment ==========
from dotenv import load_dotenv

# ========== Data Handling ==========
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, recall_score, precision_score,
    f1_score, mean_squared_error
)
from datasets import Dataset, load_dataset

# ========== Readability Metrics ==========
import textstat

# ========== Visualization ==========
import matplotlib.pyplot as plt
from tqdm import tqdm
from tqdm.auto import tqdm as tqdm_auto
from tqdm.notebook import tqdm as tqdm_notebook

# ========== PyTorch & Transformers ==========
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset as TorchDataset, DataLoader
from transformers import (
    AutoTokenizer, AutoModel, AutoConfig,
    TrainingArguments, Trainer,
    AutoModelForCausalLM, pipeline
)

# ========== Hugging Face Auth ==========
from huggingface_hub import login

# model_ai list: mistralai/Mistral-7B-Instruct-v0.3, meta-llama/Meta-Llama-3.1-8B-Instruct
#mmlu_topics ["college_computer_science", "college_chemistry"]

CACHE = '/'
DATACACHE = '/'

data_path = "/"
num_return_sequences = 5

# 14. Create a Folder to Save the Plot (If Needed)
os.makedirs(data_path, exist_ok=True)

# Suppress Warnings
warnings.filterwarnings('ignore')
tqdm.pandas()

# Load environment variables from the .env file one level up
env_path = '../.env'
load_dotenv(dotenv_path=env_path)

# Retrieve the token from environment variables
login(token="your-token-here", add_to_git_credential=False)

# List of model that are used in the experiment
models_ids = ["Qwen/Qwen2.5-0.5B"]

# It was proposed to have specific text lengths to iterate
OUTPUT_LENGTHS = [50, 100, 150, 200]

# Defined readability levels. I decided to group school grades into school levels.
READABILITY_LEVELS = [
    "Early Elementary",
    "Upper Elementary",
    "Middle School",
    "High School",
    "College",
    "Advanced College",
    "Graduate",
    "Research"
]

# First 5 topics are predifined for the experiment
TOPICS = ['human aging', 'computer security', 'astronomy', 'prehistory', 'nutrition']

PROMPTING_TECHNIQUES = ["Zero-Shot", "Few-Shot", "Two-Step", "Chain-of-Thought", "Chain-of-Thought-Advanced"]
#PROMPTING_TECHNIQUES = ["Chain-of-Thought-Advanced"]
NUM_REPEATS = 10

# CLEAR corpus is used to get texts as an examples for certain readability levels. It will be
# use for Few-Shot technique, where I need examples of text of defined readability level.
df = pd.read_csv("CLEAR.csv")

# Here I search for texts based on their Flesch-Kincaid grade level, as most universal.
readability_column = "Flesch-Kincaid-Grade-Level"

# Function to map readability level
def map_readability_level(score):
    try:
        score = float(score)
        if score < 4:
            return "Early Elementary"
        elif score < 7:
            return "Upper Elementary"
        elif score < 9:
            return "Middle School"
        elif score < 13:
            return "High School"
        elif score < 15:
            return "College"
        elif score < 17:
            return "Advanced College"
        elif score < 18:
            return "Graduate"
        else:
            return "Research"
    except:
        return "Unknown"

# Apply readability mapping
df["Readability Level"] = df[readability_column].apply(map_readability_level)

# Filter out rows without level or text
df_filtered = df[(df["Readability Level"] != "Unknown") & (df["Excerpt"].notna())]

# I get two samples for readability level, it can be more, I can use it as a parameter
excerpt_samples = (
    df_filtered
    .groupby("Readability Level")
    .apply(lambda g: g.sample(n=min(2, len(g)), random_state=42)[["Excerpt", "Title", "URL"]])
    .reset_index(level=0)
    .rename(columns={"level_0": "Readability Level"})
)

# Store result
excerpt_df = excerpt_samples.copy()

# For future work of result analysis, accuracy analysis. Mapping of metrics results and levels
# I will group them, but also try to differentiate for different grade levels, to compare.
# Also some metrics are not universal, and more adapted for school grade level, like SPACHE
# this is interesting to analyze. Universal mapping helps to compare

FLESCH_KINCAID_GRADE = {
    1: "Early Elementary",
    2: "Early Elementary",
    3: "Early Elementary",
    4: "Upper Elementary",
    5: "Upper Elementary",
    6: "Upper Elementary",
    7: "Middle School",
    8: "Middle School",
    9: "High School",
    10: "High School",
    11: "High School",
    12: "High School",
    13: "College",
    14: "College",
    15: "Advanced College",
    16: "Advanced College",
    17: "Graduate",
    18: "Research"
}

FLESCH_READING_EASE = {
    (90, 100): "Upper Elementary",
    (80, 90): "Upper Elementary",
    (70, 80): "Middle School",
    (65, 70): "Middle School",
    (60, 65): "High School",
    (50, 60): "High School",
    (40, 50): "High School",
    (30, 40): "High School",
    (0, 30): "College"
}

ARI = {
    1: "Early Elementary",
    2: "Early Elementary",
    3: "Early Elementary",
    4: "Upper Elementary",
    5: "Upper Elementary",
    6: "Upper Elementary",
    7: "Middle School",
    8: "Middle School",
    9: "High School",
    10: "High School",
    11: "High School",
    12: "High School",
    13: "College",
    14: "College",
    15: "Advanced College",
    16: "Advanced College",
    17: "Graduate",
    18: "Research",
    19: "Research",
    20: "Legal / Medical"
}

COLEMAN_LIAU = {
    1: "Early Elementary",
    2: "Early Elementary",
    3: "Early Elementary",
    4: "Upper Elementary",
    5: "Upper Elementary",
    6: "Upper Elementary",
    7: "Middle School",
    8: "Middle School",
    9: "High School",
    10: "High School",
    11: "High School",
    12: "High School",
    13: "College",
    14: "College",
    15: "Advanced College",
    16: "Advanced College",
    17: "Graduate",
    18: "Research"
}

GUNNING_FOG = {
    6: "Upper Elementary",
    7: "Middle School",
    8: "Middle School",
    9: "High School",
    10: "High School",
    11: "High School",
    12: "High School",
    13: "College",
    14: "College",
    15: "Advanced College",
    16: "Advanced College",
    17: "Graduate",
    18: "Research",
    19: "Research",
    20: "Legal / Medical"
}

SMOG_INDEX = {
    6: "Upper Elementary",
    7: "Middle School",
    8: "Middle School",
    9: "High School",
    10: "High School",
    11: "High School",
    12: "High School",
    13: "College",
    14: "College",
    15: "Advanced College",
    16: "Advanced College",
    17: "Graduate",
    18: "Research",
    19: "Legal / Medical"
}

SPACHE = {
    1: "Early Elementary",
    2: "Early Elementary",
    3: "Early Elementary",
    4: "Upper Elementary"
}

LINSEAR_WRITE = {
    4: "Upper Elementary",
    5: "Upper Elementary",
    6: "Upper Elementary",
    7: "Middle School",
    8: "Middle School",
    9: "High School",
    10: "High School",
    11: "High School",
    12: "High School",
    13: "College",
    14: "College",
    15: "Advanced College",
    16: "Graduate"
}

FLESCH_KINCAID_GRADE_TARGET = {
    "Early Elementary": 2,
    "Upper Elementary": 5,
    "Middle School": 8,
    "High School": 11,
    "College": 13,
    "Advanced College": 15,
    "Graduate": 17,
    "Research": 18
}

#Universal function to get results of all metrics
def evaluate_metrics(text):
    return {
        "Flesch-Kincaid": textstat.flesch_kincaid_grade(text),
        "Flesch Ease": textstat.flesch_reading_ease(text),
        "ARI": textstat.automated_readability_index(text),
        "SMOG": textstat.smog_index(text),
        "Gunning Fog": textstat.gunning_fog(text),
        "Coleman-Liau": textstat.coleman_liau_index(text),
        "Spache": textstat.spache_readability(text),
        "Linsear Write": textstat.linsear_write_formula(text),
    }

models = []
device = "cuda" if torch.cuda.is_available() else "cpu"
for model_id in models_ids:
    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=CACHE, use_fast=True) # per mistralai/Mistral-7B-Instruct-v0.3
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        trust_remote_code=True,
        cache_dir=CACHE,
        torch_dtype=torch.float16 if device == "cuda" else torch.float32
    ).to(device)

    # Ensure pad_token_id is set to avoid warnings
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id

    model_pipeline = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        device=0 if device == "cuda" else -1,
        temperature=0.8,          # add randomness
        top_p=0.9,                # nucleus sampling
        top_k=50,                 # limit to top_k tokens
        do_sample=True,           # must be True to enable sampling
        pad_token_id=tokenizer.pad_token_id
    )
    models.append(model_pipeline)

def generate_with_transformers(prompt, model, max_new_tokens=200):
    output = model(prompt, max_new_tokens=max_new_tokens, do_sample=True)
    return output[0]["generated_text"].strip()

def get_model_name(model_obj):
    try:
        return model_obj.model.config.name_or_path
    except:
        return str(model_obj)

results = []

iterable = list(product(models, TOPICS, READABILITY_LEVELS, OUTPUT_LENGTHS, PROMPTING_TECHNIQUES, range(NUM_REPEATS)))

for model, topic, level_label, output_length, technique, run_id in tqdm(iterable, desc="Running prompting techniques"):
    model_name = get_model_name(model)

    if technique == "Zero-Shot":
        prompt = (
            f"Generate a short paragraph about '{topic}' targeting readability level "
            f"{level_label}. Limit the output to {output_length} words."
        )
        rewrite_prompt = ""

    elif technique == "Few-Shot":
        examples_df = excerpt_df[excerpt_df["Readability Level"] == level_label]
        few_shot_examples = examples_df["Excerpt"].sample(n=min(2, len(examples_df)), random_state=42).tolist()
        example_text = "\n\n".join([f"Example {i+1}:\n{ex}" for i, ex in enumerate(few_shot_examples)])

        prompt = (
            f"{example_text}\n\n"
            f"Using vocabulary and structure from examples, write a short paragraph about '{topic}' "
            f"targeting the same readability level ({level_label}), "
            f"limited to {output_length} words."
        )
        rewrite_prompt = ""

    elif technique == "Two-Step":
        summary_prompt = f"Write a summary about '{topic}'. Limit the output to {output_length} words."
        summary = generate_with_transformers(summary_prompt, model)

        rewrite_prompt = (
            f"Rewrite the following text to match the readability level of {level_label} "
            f"and limit the output to {output_length} words:\n\n{summary}"
        )
        prompt = summary_prompt
        output = generate_with_transformers(rewrite_prompt, model)

    elif technique == "Chain-of-Thought":
        prompt = (
            f"Step 1: Think about how to explain '{topic}' to someone reading at the {level_label} level.\n"
            f"Step 2: Select appropriate vocabulary and sentence structure.\n"
            f"Step 3: Write a paragraph of {output_length} words that follows your plan."
        )
        rewrite_prompt = ""

    elif technique == "Chain-of-Thought-Advanced":
        level_numeric = FLESCH_KINCAID_GRADE_TARGET[level_label]
        prompt = (
            f"Step 1: Think about how to explain '{topic}' in {output_length} words.\n"
            f"Step 2: Select appropriate vocabulary and sentence structure, so it will target "
            f"Flesch-Kincaid Grade Level of approximately {level_numeric}. "
            f"The Flesch-Kincaid Grade Level is calculated as:\n"
            f"0.39 × (total words / total sentences) + 11.8 × (total syllables / total words) - 15.59.\n"
            f"Step 3: Make sure the output has correct metric value.\n"
            f"Step 4: Follow your plan and write the final paragraph."
        )
        rewrite_prompt = ""

    if technique != "Two-Step":
        output = generate_with_transformers(prompt, model)

    metrics = evaluate_metrics(output)
    # Prints to check the flow, they can be commented out
    #print("\n")
    #print(f"Run_Id: {run_id}")
    #print(f"Technique: {technique}")
    #print(f"Topic: {topic}")
    #print(f"Target Level: {level_label}")
    #print(f"Output Length: {output_length}")
    #print(f"Model Name: {model_name}")
    #print(f"Prompt ({technique}): {prompt}")
    #if rewrite_prompt:
    #    print(f"\nRewrite Prompt: {rewrite_prompt}")
    #print(f"Output:\n{output}")
    #print("Metrics:")
    #for k, v in metrics.items():
    #    print(f"  {k}: {v:.2f}")

    results.append({
        "Technique": technique,
        "Topic": topic,
        "Target Level": level_label,
        "Output Length": output_length,
        "Model": model_name,
        "Prompt": prompt,
        "Rewrite Prompt": rewrite_prompt,
        "Text": output,
        **metrics
    })

results_df = pd.DataFrame(results)
results_df.to_csv("experiment_results.csv", index=False)